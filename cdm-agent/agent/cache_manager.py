"""
Narrative cache management for permanent storage
Handles storing and retrieving generated narratives from PostgreSQL
"""
import hashlib
import json
from typing import Optional, Dict, Any
from common.db import conn, q, one, execute

def generate_cache_key(narrative_type: str, trade_id: str, event_id: Optional[str] = None) -> str:
    """
    Generate cache key for narrative lookup
    
    Args:
        narrative_type: 'trade' or 'event'
        trade_id: Trade identifier
        event_id: Event identifier (for event narratives)
    
    Returns:
        Cache key string
    """
    if narrative_type == 'trade':
        return f"trade:{trade_id}"
    elif narrative_type == 'event':
        if not event_id:
            raise ValueError("event_id required for event narratives")
        return f"event:{trade_id}:{event_id}"
    else:
        raise ValueError(f"Invalid narrative_type: {narrative_type}")

def generate_version_hash(timeline_data: Any) -> str:
    """
    Generate version hash from trade timeline data for cache invalidation
    
    Args:
        timeline_data: Trade timeline or event data
    
    Returns:
        SHA256 hash of the data
    """
    data_str = json.dumps(timeline_data, sort_keys=True)
    return hashlib.sha256(data_str.encode()).hexdigest()

def get_narrative(cache_key: str) -> Optional[Dict[str, Any]]:
    """
    Retrieve narrative from cache by cache key
    
    Args:
        cache_key: Cache key generated by generate_cache_key()
    
    Returns:
        Dictionary with narrative data or None if not found
        {
            'narrative_text': str,
            'generation_metadata': dict,
            'created_at': datetime,
            'updated_at': datetime
        }
    """
    cnx = conn()
    try:
        result = one(
            cnx,
            """
            SELECT 
                narrative_text,
                generation_metadata,
                created_at,
                updated_at,
                version_hash
            FROM narrative_cache
            WHERE cache_key = %s
            """,
            (cache_key,)
        )
        
        if result:
            return {
                'narrative_text': result['narrative_text'],
                'generation_metadata': result['generation_metadata'],
                'created_at': result['created_at'],
                'updated_at': result['updated_at'],
                'version_hash': result['version_hash']
            }
        return None
    finally:
        cnx.close()

def save_narrative(
    cache_key: str,
    narrative_type: str,
    trade_id: str,
    narrative_text: str,
    generation_metadata: Dict[str, Any],
    event_id: Optional[str] = None,
    perspective: Optional[str] = None,
    version_hash: Optional[str] = None
) -> bool:
    """
    Save narrative to permanent storage
    
    Args:
        cache_key: Cache key for lookup
        narrative_type: 'trade' or 'event'
        trade_id: Trade identifier
        narrative_text: Generated narrative text
        generation_metadata: Metadata about generation (tool calls, tokens, etc.)
        event_id: Event identifier (for event narratives)
        perspective: Perspective name (for trade narratives)
        version_hash: Version hash for cache invalidation
    
    Returns:
        True if saved successfully
    """
    cnx = conn()
    try:
        # Use INSERT ... ON CONFLICT UPDATE to handle duplicates
        execute(
            cnx,
            """
            INSERT INTO narrative_cache (
                cache_key,
                narrative_type,
                trade_id,
                event_id,
                perspective,
                narrative_text,
                generation_metadata,
                version_hash,
                updated_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT (cache_key) DO UPDATE SET
                narrative_text = EXCLUDED.narrative_text,
                generation_metadata = EXCLUDED.generation_metadata,
                version_hash = EXCLUDED.version_hash,
                updated_at = NOW()
            """,
            (
                cache_key,
                narrative_type,
                trade_id,
                event_id,
                perspective,
                narrative_text,
                json.dumps(generation_metadata),
                version_hash
            )
        )
        return True
    finally:
        cnx.close()

def invalidate_trade_narratives(trade_id: str) -> int:
    """
    Invalidate all narratives for a trade (useful when new events are added)
    
    Args:
        trade_id: Trade identifier
    
    Returns:
        Number of narratives deleted
    """
    cnx = conn()
    try:
        result = q(
            cnx,
            "DELETE FROM narrative_cache WHERE trade_id = %s RETURNING id",
            (trade_id,)
        )
        return len(result)
    finally:
        cnx.close()

def get_trade_narrative(trade_id: str) -> Optional[Dict[str, Any]]:
    """
    Convenience method to get trade-level narrative
    
    Args:
        trade_id: Trade identifier
    
    Returns:
        Narrative data or None
    """
    cache_key = generate_cache_key('trade', trade_id)
    return get_narrative(cache_key)

def get_event_narrative(trade_id: str, event_id: str) -> Optional[Dict[str, Any]]:
    """
    Convenience method to get event-level narrative
    
    Args:
        trade_id: Trade identifier
        event_id: Event identifier
    
    Returns:
        Narrative data or None
    """
    cache_key = generate_cache_key('event', trade_id, event_id=event_id)
    return get_narrative(cache_key)

def save_trade_narrative(
    trade_id: str,
    narrative_text: str,
    generation_metadata: Dict[str, Any],
    version_hash: Optional[str] = None
) -> bool:
    """
    Convenience method to save trade-level narrative
    
    Args:
        trade_id: Trade identifier
        narrative_text: Generated narrative
        generation_metadata: Metadata about generation
        version_hash: Version hash for invalidation
    
    Returns:
        True if saved successfully
    """
    cache_key = generate_cache_key('trade', trade_id)
    return save_narrative(
        cache_key=cache_key,
        narrative_type='trade',
        trade_id=trade_id,
        narrative_text=narrative_text,
        generation_metadata=generation_metadata,
        perspective=None,
        version_hash=version_hash
    )

def save_event_narrative(
    trade_id: str,
    event_id: str,
    narrative_text: str,
    generation_metadata: Dict[str, Any],
    version_hash: Optional[str] = None
) -> bool:
    """
    Convenience method to save event-level narrative
    
    Args:
        trade_id: Trade identifier
        event_id: Event identifier
        narrative_text: Generated narrative
        generation_metadata: Metadata about generation
        version_hash: Version hash for invalidation
    
    Returns:
        True if saved successfully
    """
    cache_key = generate_cache_key('event', trade_id, event_id=event_id)
    return save_narrative(
        cache_key=cache_key,
        narrative_type='event',
        trade_id=trade_id,
        narrative_text=narrative_text,
        generation_metadata=generation_metadata,
        event_id=event_id,
        version_hash=version_hash
    )

def save_narrative_logs(
    cache_key: str,
    narrative_type: str,
    trade_id: str,
    logs: list,
    event_id: Optional[str] = None
) -> bool:
    """
    Save log messages for a narrative generation process
    
    Args:
        cache_key: Cache key matching the narrative
        narrative_type: 'trade' or 'event'
        trade_id: Trade identifier
        logs: List of log dictionaries with 'type', 'message', 'metadata'
        event_id: Event identifier (for event narratives)
    
    Returns:
        True if saved successfully
    """
    cnx = conn()
    try:
        # Delete existing logs for this cache_key first
        execute(
            cnx,
            "DELETE FROM narrative_logs WHERE cache_key = %s",
            (cache_key,)
        )
        
        # Insert new logs
        for index, log in enumerate(logs):
            execute(
                cnx,
                """
                INSERT INTO narrative_logs (
                    cache_key,
                    narrative_type,
                    trade_id,
                    event_id,
                    log_index,
                    log_type,
                    message,
                    metadata
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (
                    cache_key,
                    narrative_type,
                    trade_id,
                    event_id,
                    index,
                    log.get('type'),
                    log.get('message', ''),
                    json.dumps(log.get('metadata', {})) if log.get('metadata') else None
                )
            )
        return True
    finally:
        cnx.close()

def get_narrative_logs(cache_key: str) -> list:
    """
    Retrieve log messages for a narrative
    
    Args:
        cache_key: Cache key matching the narrative
    
    Returns:
        List of log dictionaries ordered by log_index
    """
    cnx = conn()
    try:
        results = q(
            cnx,
            """
            SELECT log_index, log_type, message, metadata, timestamp
            FROM narrative_logs
            WHERE cache_key = %s
            ORDER BY log_index ASC
            """,
            (cache_key,)
        )
        
        logs = []
        for row in results:
            logs.append({
                'type': row['log_type'],
                'message': row['message'],
                'metadata': row['metadata'] if row['metadata'] else {},
                'timestamp': str(row['timestamp']) if row['timestamp'] else None
            })
        return logs
    finally:
        cnx.close()
